{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lego_TrainVGG16_Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrFlexi/LegoPredict/blob/master/Lego_TrainVGG16_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2AHFaZ_c9-",
        "colab_type": "code",
        "outputId": "b8cbb506-2b71-4d42-890a-fff32bf052aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "!git clone https://github.com/MrFlexi/LegoPredict.git\n",
        "\n",
        "!ls\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# Tensorboard\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LegoPredict'...\n",
            "remote: Enumerating objects: 4189, done.\u001b[K\n",
            "remote: Total 4189 (delta 0), reused 0 (delta 0), pack-reused 4189\u001b[K\n",
            "Receiving objects: 100% (4189/4189), 44.36 MiB | 29.06 MiB/s, done.\n",
            "Resolving deltas: 100% (507/507), done.\n",
            "LegoPredict  sample_data\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "--2020-04-21 07:33:27--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.232.135.84, 52.3.79.57, 34.197.27.35, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.232.135.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  37.3MB/s    in 0.4s    \n",
            "\n",
            "2020-04-21 07:33:28 (37.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1oMBPIaf9Z_",
        "colab_type": "code",
        "outputId": "1ff6b1ab-7210-4ae0-b63b-35268cd2a125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Tensorboard get URL \n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://8e853e8c.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCkaMJwp_a8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "e841651d-a615-405c-c131-6ad8c372411c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import_flowers = 'No' #@param [\"No\", \"Yes\"]\n",
        "image_sources  = 'lego_fotos' #@param {type:\"string\"} \n",
        "image_size = 224 #@param {type:\"integer\"} \n",
        "number_classes = 6 #@param {type:\"integer\"} \n",
        "number_epochs =   2#@param {type:\"integer\"} \n",
        "save_model_path = 'LegoTrainedVGG16_15Layer_classes6_epochs_40.h5' #@param {type:\"string\"}\n",
        "best_model_path = 'LegoTrainedVGG16_15Layer_classes6_best_model.h5' #@param {type:\"string\"} \n",
        "labels_file     = 'labels_classes6.json' #@param {type:\"string\"} \n",
        "train_batchsize = 50 #@param {type:\"integer\"} \n",
        "val_batchsize = 50 #@param {type:\"integer\"} \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os   # set working directory to \n",
        "import json\n",
        "os.chdir(\"/content/LegoPredict\")\n",
        "!ls\n",
        "#!wget https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5  -P /content/keras-retinanet/snapshots/\n",
        "\n",
        "\n",
        "if import_flowers == \"Yes\":\n",
        "    os.chdir(\"/content/LegoPredict/lego_fotos/train\")\n",
        "    !wget http://download.tensorflow.org/example_images/flower_photos.tgz \n",
        "    !tar -xf flower_photos.tgz\n",
        "    os.chdir(\"/content/LegoPredict/\")\n",
        "    !cp -r ./lego_fotos/train/flower_photos/daisy/ ./lego_fotos/train/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/dandelion/ ./lego_fotos/train/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/roses/ ./lego_fotos/train/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/sunflowers/ ./lego_fotos/train/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/tulips/ ./lego_fotos/train/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/daisy/ ./lego_fotos/validation/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/dandelion/ ./lego_fotos/validation/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/roses/ ./lego_fotos/validation/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/sunflowers/ ./lego_fotos/validation/\n",
        "    !cp -r ./lego_fotos/train/flower_photos/tulips/ ./lego_fotos/validation/\n",
        "    !rm -r ./lego_fotos/train/flower_photos/\n",
        "    !rm ./lego_fotos/train/flower_photos.tgz\n",
        "    os.chdir(\"/content/LegoPredict\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CamMarkFeature.py      lego_fotos\t      settings_dialog.py\n",
            "candle.jpg\t       models\t\t      settings.json\n",
            "CaptureImageWebcam.py  PredictBatch.py\t      test.py\n",
            "fineTuning4.py\t       PredictFromFolder2.py  test_txt.py\n",
            "fineTuning.py\t       PredictFromFolder.py   TrainInceptionV3.py\n",
            "icon.gif\t       PredictFromWebcam.py   TrainMobilnet_Transfer.py\n",
            "labels.json\t       Predict.py\t      UseFinetunedLego.py\n",
            "lables.txt\t       result.json\t      UsePrettrainedApp.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WgyEA2NGxUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cp /content/gdrive/My*Drive/keras-retinanet/images/* /content/keras-retinanet/images/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaN7EXehAcyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY1pwuxK1Hek",
        "colab_type": "code",
        "outputId": "ca12cd50-8f1f-4764-edbb-676464ab2b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "import time\n",
        "\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()\n",
        "\n",
        "callbacks= [ReduceLROnPlateau(monitor='val_acc', \n",
        "                                        patience=5, \n",
        "                                        verbose=1, \n",
        "                                        factor=0.5, \n",
        "                                        min_lr=0.00001),\n",
        "            \n",
        "           TensorBoard(log_dir='./log', histogram_freq=0,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=train_batchsize,\n",
        "                         write_images=True),\n",
        "            \n",
        "           EarlyStopping(monitor='val_loss', patience=10 ),\n",
        "           ModelCheckpoint(filepath=best_model_path, monitor='val_loss', save_best_only=True)\n",
        "           ]\n",
        "\n",
        "train_dir = \"./\" + image_sources + \"/train\"\n",
        "validation_dir = \"./\" + image_sources + \"/validation\"\n",
        "prediction_dir = \"./\" + image_sources + \"/predict\"\n",
        "\n",
        "\n",
        "# Change the batchsize according to your system RAM\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "\n",
        "#Load the VGG model\n",
        "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "# Freeze all the layers\n",
        "for layer in vgg_conv.layers[:-15]:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "# Create the model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add the vgg convolutional base model\n",
        "model.add(vgg_conv)\n",
        "\n",
        "# Add new layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(number_classes, activation='softmax'))\n",
        "\n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "#model.summary()\n",
        "\n",
        "# Check the trainable status of the individual layers\n",
        "for layer in vgg_conv.layers:\n",
        "    print(layer, layer.trainable)\n",
        "\n",
        "\n",
        "#  TRAIN\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "\n",
        "# Data Generator for Training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Get the label to class mapping from the generator\n",
        "label2index = train_generator.class_indices\n",
        "\n",
        "# Getting the mapping from class index to class label\n",
        "idx2label = dict((v, k) for k, v in label2index.items())\n",
        "\n",
        "print (idx2label)\n",
        "with open(labels_file, 'w') as fp:\n",
        "    json.dump(idx2label, fp)\n",
        "\n",
        "\n",
        "\n",
        "# Data Generator for Validation data\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Train the Model\n",
        "# NOTE that we have multiplied the steps_per_epoch by 2. This is because we are using data augmentation.\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=2*train_generator.samples/train_generator.batch_size ,\n",
        "      epochs=number_epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1,\n",
        "      callbacks = callbacks)\n",
        "\n",
        "# Save the Model\n",
        "model.save(save_model_path)\n",
        "\n",
        "!cp /content/LegoPredict/*.h5 \"/content/gdrive/My Drive/keras-retinanet/snapshots/\"\n",
        "\n",
        "# Plot the accuracy and loss curves\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------\n",
        "#       Predict\n",
        "#----------------------------------------------------------------\n",
        "\n",
        "# Create a generator for prediction\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    prediction_dir,\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=val_batchsize,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)\n",
        "\n",
        "# Get the filenames from the generator\n",
        "fnames = validation_generator.filenames\n",
        "\n",
        "# Get the ground truth from generator\n",
        "ground_truth = validation_generator.classes\n",
        "\n",
        "# Get the label to class mapping from the generator\n",
        "label2index = validation_generator.class_indices\n",
        "\n",
        "# Getting the mapping from class index to class label\n",
        "idx2label = dict((v, k) for k, v in label2index.items())\n",
        "\n",
        "# Get the predictions from the model using the generator\n",
        "predictions = model.predict_generator(validation_generator,\n",
        "                                      steps=validation_generator.samples / validation_generator.batch_size, verbose=1)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "errors = np.where(predicted_classes != ground_truth)[0]\n",
        "print(\"No of errors = {}/{}\".format(len(errors), validation_generator.samples))\n",
        "\n",
        "# Show the errors\n",
        "for i in range(len(errors)):\n",
        "    pred_class = np.argmax(predictions[errors[i]])\n",
        "    pred_label = idx2label[pred_class]\n",
        "\n",
        "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
        "        fnames[errors[i]].split('/')[0],\n",
        "        pred_label,\n",
        "        predictions[errors[i]][pred_class])\n",
        "\n",
        "    original = load_img('{}/{}'.format(prediction_dir, fnames[errors[i]]))\n",
        "    plt.figure(figsize=[7, 7])\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.imshow(original)\n",
        "    plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2f71522f2855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m callbacks= [ReduceLROnPlateau(monitor='val_acc', \n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_available_gpus\u001b[0;34m()\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_list_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'device:gpu'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.config' has no attribute 'experimental_list_devices'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWs6NKD0AfFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2wbgRCMIZT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/LegoPredict/*.h5 \"/content/gdrive/My Drive/keras-retinanet/snapshots/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24qY-lrrR4lG",
        "colab_type": "code",
        "outputId": "8bb90185-5f90-4916-d868-2e67f7484c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "# https://engmrk.com/kerasapplication-pre-trained-model/\n",
        "\n",
        "save_model_path = 'LegoTrainedVGG16_15Layer_classes6_best_model.h5' #@param {type:\"string\"} \n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from keras.applications import inception_v3\n",
        "from keras.models \t\timport load_model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "\n",
        "inception_model = inception_v3.InceptionV3(weights=\"imagenet\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from pygments.lexer import include\n",
        "\n",
        "#os.environ[\"HTTPS_PROXY\"] = \"http://proxy.le.grp:8080\"\n",
        "\n",
        "def read_model():\n",
        "\t\n",
        "\tprint(model.summary())\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def show_results():\n",
        "    # Show the results\n",
        "    for i in range(len(predicted_classes)):\n",
        "        pred_class = np.argmax(predictions[predicted_classes[i]])\n",
        "        pred_label = idx2label[pred_class]\n",
        "\n",
        "        title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
        "            fnames[predicted_classes[i]].split('/')[0],\n",
        "            pred_label,\n",
        "            predictions[predicted_classes[i]][pred_class])\n",
        "\n",
        "        original = load_img('{}/{}'.format(validation_dir, fnames[predicted_classes[i]]))\n",
        "        plt.figure(figsize=[7, 7])\n",
        "        plt.axis('off')\n",
        "        plt.title(title)\n",
        "        plt.imshow(original)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def get_pil_image(filename):\n",
        "    # load an image in PIL format\n",
        "    original_image = load_img(filename, target_size=(224, 224))\n",
        "\n",
        "    # convert the PIL image (width, height) to a NumPy array (height, width, channel)\n",
        "    numpy_image = img_to_array(original_image)\n",
        "\n",
        "    # Convert the image into 4D Tensor (samples, height, width, channels) by adding an extra dimension to the axis 0.\n",
        "    input_image = np.expand_dims(numpy_image, axis=0)\n",
        "\n",
        "    print(\"PIL image size = \", original_image.size)\n",
        "    print(\"NumPy image size = \", numpy_image.shape)\n",
        "    print(\"Input image size = \", input_image.shape)\n",
        "    plt.imshow(np.uint8(input_image[0]))\n",
        "    return input_image\n",
        "\n",
        "\n",
        "\n",
        "def predict_inception_v3( input_image ):\n",
        "    # Load the Inception_V3 model\n",
        "\n",
        "\n",
        "    # preprocess for inception_v3\n",
        "    processed_image_inception_v3 = inception_v3.preprocess_input(input_image.copy())\n",
        "\n",
        "    # inception_v3\n",
        "    start = time.time()\n",
        "    predictions_inception_v3 = inception_model.predict(processed_image_inception_v3)\n",
        "    label_inception_v3 = decode_predictions(predictions_inception_v3)\n",
        "    print(\"label_inception_v3 = \", label_inception_v3)\n",
        "    ende = time.time()\n",
        "    print('{:5.3f}s'.format(ende - start))\n",
        "\n",
        "def predict_mymodel( img_path ):\n",
        "\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img.show()\n",
        "    print(type(img))\n",
        "    print(img.format)\n",
        "    print(img.mode)\n",
        "    print(img.size)\n",
        "    img_tensor = image.img_to_array(img)  # (height, width, channels)\n",
        "    img_tensor = np.expand_dims(img_tensor,axis=0)  # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
        "    img_tensor /= 255.  # imshow expects values in the range [0, 1]\n",
        "    \n",
        "\n",
        "    start = time.time()\n",
        "    predictions_my = vgg16_model.predict(img_tensor)\n",
        "    #print(predictions_my)\n",
        "    # label_MY = decode_predictions(predictions_my)\n",
        "    # print(\"My labels = \", label_MY )\n",
        "    ende = time.time()\n",
        "    y_classes = predictions_my.argmax(axis=-1)\n",
        "    print('Best Match', labels[y_classes[0]],predictions_my[0,y_classes[0]],'{:5.3f}s'.format(ende - start))\n",
        "\n",
        "\n",
        "# convert JSON key field string to DICT Integer\n",
        "def jsonKeys2int(x):\n",
        "      if isinstance(x, dict):\n",
        "        return {int(k):v for k,v in x.items()}\n",
        "      return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "    save_model_path = \"./\" + save_model_path\n",
        "\n",
        "    # load model\n",
        "    vgg16_model = load_model(save_model_path)\n",
        "    \n",
        "    # load labels\n",
        "    labels_file = \"./\" + labels_file\n",
        "    with open(labels_file) as f:\n",
        "      labels = json.load(f,object_hook=jsonKeys2int)\n",
        "    print(labels)    \n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    filename_3x5LGreen = \"./lego_fotos/predict/3x5LGreen/frame20190226-213256.jpg\"\n",
        "    filename_3x5LGreen_Band = \"./lego_fotos/predict/3x5LGreen/3x5LGreen_Band.jpg\"\n",
        "    filename_gear = \"./lego_fotos/train/Gear20Beige/frame20190222-16421620.jpg\"\n",
        "  \n",
        "\n",
        "    \n",
        "    filename_3x5LOrange = \"./lego_fotos/predict/3x5LOrange/frame20190226-213949101.jpg\"\n",
        "    \n",
        "    filename_3x5LRed =  \"./lego_fotos/train/3x5LRed/frame20190222-1629518.jpg\" \n",
        "    \n",
        "    filename_1x4LRed = \"./lego_fotos/train/1x4LRed/frame20190204-1642049.jpg\"\n",
        "     \n",
        " \n",
        "    \n",
        "    filename_sunflower = \"./lego_fotos/unsorted/sunflower.jpg\"\n",
        "  \n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------------------------\n",
        "    # Predict SingleImage via Inception V3\n",
        "    #---------------------------------------------------------------------------------------------\n",
        "\n",
        "    #print(\"Inception - Lego Brick \")\n",
        "    #predict_inception_v3( lego_brick )\n",
        "    #print(\"Inception - Lego Connector Black \")\n",
        "    #predict_inception_v3( image_rose )\n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------------------------\n",
        "    # My Trained Model - Single Image\n",
        "    #---------------------------------------------------------------------------------------------\n",
        "    # processed_image_my = my_model.preprocess_input(input_image.copy())\n",
        "\n",
        "    print(\"VGG16 - 3x5LGreen \")\n",
        "    predict_mymodel(filename_3x5LGreen)\n",
        "    \n",
        "    \n",
        "    print(\"VGG16 - 3x5LGreen Band \")\n",
        "    predict_mymodel(filename_3x5LGreen_Band)\n",
        "    \n",
        "    \n",
        "    print(\"VGG16 - Gear \")\n",
        "    predict_mymodel(filename_gear)\n",
        "    \n",
        "\n",
        "    print(\"VGG16-  3x5LOrange \")\n",
        "    predict_mymodel(filename_3x5LOrange)\n",
        "    \n",
        "    print(\"VGG16--  3x5LRed \")\n",
        "    predict_mymodel(filename_3x5LRed)\n",
        "   \n",
        "    \n",
        "    \n",
        "    print(\"VGG16-  Sunflower \")\n",
        "    predict_mymodel(filename_sunflower)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '3x5LBlack', 1: '3x5LGreen', 2: '3x5LOrange', 3: '3x5LRed', 4: 'BeltGray', 5: 'Gear20Beige'}\n",
            "VGG16 - 3x5LGreen \n",
            "<class 'PIL.Image.Image'>\n",
            "None\n",
            "RGB\n",
            "(224, 224)\n",
            "Best Match 3x5LGreen 1.0 7.032s\n",
            "VGG16 - 3x5LGreen Band \n",
            "<class 'PIL.Image.Image'>\n",
            "None\n",
            "RGB\n",
            "(224, 224)\n",
            "Best Match 3x5LGreen 1.0 0.014s\n",
            "VGG16 - Gear \n",
            "<class 'PIL.Image.Image'>\n",
            "None\n",
            "RGB\n",
            "(224, 224)\n",
            "Best Match Gear20Beige 1.0 0.011s\n",
            "VGG16-  3x5LOrange \n",
            "<class 'PIL.Image.Image'>\n",
            "None\n",
            "RGB\n",
            "(224, 224)\n",
            "Best Match 3x5LOrange 1.0 0.011s\n",
            "VGG16--  3x5LRed \n",
            "<class 'PIL.Image.Image'>\n",
            "None\n",
            "RGB\n",
            "(224, 224)\n",
            "Best Match 3x5LRed 1.0 0.011s\n",
            "VGG16-  Sunflower \n",
            "<class 'PIL.Image.Image'>\n",
            "None\n",
            "RGB\n",
            "(224, 224)\n",
            "Best Match 3x5LOrange 1.0 0.011s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovqoA5xvScRY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gbNNh1zSeyw",
        "colab_type": "code",
        "outputId": "cb19c4ea-57b5-450e-8b6a-e024f152fcc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CamMarkFeature.py\t\t\t\t PredictBatch.py\n",
            "candle.jpg\t\t\t\t\t PredictFromFolder2.py\n",
            "CaptureImageWebcam.py\t\t\t\t PredictFromFolder.py\n",
            "fineTuning4.py\t\t\t\t\t PredictFromWebcam.py\n",
            "fineTuning.py\t\t\t\t\t Predict.py\n",
            "icon.gif\t\t\t\t\t result.json\n",
            "labels_classes6.json\t\t\t\t settings_dialog.py\n",
            "labels.json\t\t\t\t\t settings.json\n",
            "lables.txt\t\t\t\t\t test.py\n",
            "lego_fotos\t\t\t\t\t test_txt.py\n",
            "LegoTrainedVGG16_15Layer_classes6_best_model.h5  TrainInceptionV3.py\n",
            "LegoTrainedVGG16_15Layer_classes6_epochs_10.h5\t TrainMobilnet_Transfer.py\n",
            "LegoTrainedVGG16_15Layer_classes6_epochs_40.h5\t UseFinetunedLego.py\n",
            "log\t\t\t\t\t\t UsePrettrainedApp.py\n",
            "models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQywhAR3Vv-E",
        "colab_type": "code",
        "outputId": "09a3f032-c495-470a-c477-d0c04e47a2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "# Concert Model into TF Lite Model\n",
        "\n",
        "import tensorflow as tf\n",
        "os.chdir(\"/content/LegoPredict\")\n",
        "\n",
        "save_model_path = 'LegoTrainedVGG16_15Layer_classes6_best_model.h5' #@param {type:\"string\"} \n",
        "\n",
        "model = load_model(save_model_path)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c2673946f671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converted_model.tflite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'TFLiteConverter' has no attribute 'from_keras_model'"
          ]
        }
      ]
    }
  ]
}